name: Daily Job Scraper
on:
  schedule:
    - cron: '0 14 * * *'
  workflow_dispatch:
jobs:
  scrape-jobs:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests
      
      - name: Download previous database
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: jobs-database
          path: .
      
      - name: Run job scraper
        env:
          JSEARCH_API_KEY: ${{ secrets.JSEARCH_API_KEY }}
          SMTP_SERVER: ${{ secrets.SMTP_SERVER }}
          SMTP_PORT: ${{ secrets.SMTP_PORT }}
          SENDER_EMAIL: ${{ secrets.SENDER_EMAIL }}
          SENDER_PASSWORD: ${{ secrets.SENDER_PASSWORD }}
          RECIPIENT_EMAIL: ${{ secrets.RECIPIENT_EMAIL_BRIDGIT }},${{ secrets.RECIPIENT_EMAIL_COURTNEY }}
        run: run: python combined_scraper.py
      
      - name: Upload database artifact
        uses: actions/upload-artifact@v4
        with:
          name: jobs-database
          path: jobs.db
          retention-days: 90
      
      - name: Upload scrape summary
        uses: actions/upload-artifact@v4
        with:
          name: scrape-summary-${{ github.run_number }}
          path: scrape_summary.json
          retention-days: 30
